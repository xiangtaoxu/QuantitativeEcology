{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monthly-radar",
   "metadata": {},
   "source": [
    "# BIOEE 4940 : **Introduction to Quantitative Analysis in Ecology**\n",
    "### ***Spring 2021***\n",
    "### Instructor: **Xiangtao Xu** ( ✉️ xx286@cornell.edu)\n",
    "### Teaching Assistant: **Yanqiu (Autumn) Zhou** (✉️ yz399@cornell.edu)\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:royalblue\">Lecture 6</span> *Regression III: Model Selection*\n",
    "*Partly adapted from [How to be a quantitative ecologist](https://www.researchgate.net/publication/310239832_How_to_be_a_Quantitative_Ecologist_The_'A_to_R'_of_Green_Mathematics_and_Statistics) and [All of Statistics](https://www.stat.cmu.edu/~larry/all-of-statistics/)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and prepare data to be used\n",
    "# import packages and read the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "baad_data_url = 'https://raw.githubusercontent.com/xiangtaoxu/QuantitativeEcology/main/Lab1/baad_data.csv'\n",
    "baad_dictionary_url = 'https://raw.githubusercontent.com/xiangtaoxu/QuantitativeEcology/main/Lab1/baad_dictionary.csv'\n",
    "\n",
    "df_data = pd.read_csv(baad_data_url, encoding='latin_1') # can also read local files\n",
    "df_dict = pd.read_csv(baad_dictionary_url, encoding='latin_1')\n",
    "\n",
    "df_ms = df_data[['a.lf','d.bh','h.t','r.st','ma.ilf','species','family','map','mat']].dropna()\n",
    "\n",
    "# rename columns to avoid mis-interpreation for formula-based regressions\n",
    "df_ms.rename(columns={'d.bh' : 'dbh','h.t' : 'h','r.st':'wd','ma.ilf':'lma','a.lf' : 'la'},inplace=True)\n",
    "\n",
    "df_ms.dropna(inplace=True)\n",
    "\n",
    "df_ms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-python",
   "metadata": {},
   "source": [
    "#### 1. Goal of model selection (Tredennick et al. 2021):\n",
    "    \n",
    "  * Exploration:\n",
    "  \n",
    "      What are possible candidate covariates to include?\n",
    "      \n",
    "      Hypotheses generation based on inductive reasoning (e.g. Is $X_i$ influencing Y).\n",
    "      \n",
    "      Trade-off: thorough vs false discoveries (type I error)\n",
    "      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_ms[['la','dbh','h','wd','lma','map','mat']])\n",
    "plt.show()\n",
    "# which variables should be included to predict la?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of type I error for casting a wide net for explanatory variables\n",
    "# recall what is type I error\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# generate 10 random samples\n",
    "data = []\n",
    "group = []\n",
    "for i in range(20):\n",
    "    data.extend(np.random.rand(20))\n",
    "    group.extend(np.ones((20,)) * i)\n",
    "    \n",
    "rand_df = pd.DataFrame({'data' : data, 'group' : group})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 \n",
    "# conduct a pairwise correlation analysis\n",
    "mcomp = MultiComparison(rand_df['data'],rand_df['group'])\n",
    "\n",
    "res = mcomp.allpairtest(stats.pearsonr,alpha=0.05,method='Holm')\n",
    "print(res[0])\n",
    "\n",
    "pvals = np.array(res[0].data)[1::,3].astype(float)\n",
    "\n",
    "print(np.sum(pvals <= 0.05) / len(pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-following",
   "metadata": {},
   "source": [
    "      \n",
    "  * Inference:\n",
    "    \n",
    "      Test detailed hypotheses in more rigorous ways.\n",
    "      \n",
    "      Is $X_i$ and the underlying natural process an important explanatory factor for Y?\n",
    "      \n",
    "      What is the senstivity of Y to $X_i$?\n",
    "      \n",
    "      Usually involve comparing alternative models.\n",
    "      \n",
    "      Need replication and validation across a range of conditions before being accepted as scientific fact\n",
    "      \n",
    "      Collinearity can be a major challenge.\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of wd effect\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# no wd\n",
    "res0 = smf.ols('h ~ dbh',data=df_ms).fit()\n",
    "print(res0.summary())\n",
    "\n",
    "\n",
    "# wd influences intercept\n",
    "res1 = smf.ols('h ~ dbh + wd',data=df_ms).fit()\n",
    "print(res1.summary())\n",
    "\n",
    "# wd influences slope\n",
    "res2 = smf.ols('h ~ dbh + dbh:wd',data=df_ms).fit()\n",
    "print(res2.summary())\n",
    "\n",
    "# wd influences both\n",
    "res3 = smf.ols('h ~ dbh * wd',data=df_ms).fit()\n",
    "print(res3.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to tell including one extra variable is statistically meaningful?\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "print(anova_lm(res0,res1,res2,res3))\n",
    "# likelihood test\n",
    "# res.compare_lr_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3.compare_lr_test(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-flower",
   "metadata": {},
   "source": [
    "  * Prediction:\n",
    "  \n",
    "      One of the most common goal of statistical models (e.g. statistical down-scaling, ecological forecasting)\n",
    "      \n",
    "      Given the equation $\\hat{Y} = \\hat{\\beta}X$ acquired from regression, prediction is more about $\\hat{Y}$ while inference is more about $\\hat{\\beta}$\n",
    "      \n",
    "      Confronting model predictions with new data is the ultimate test of our understanding --> the importance of 'out of sample' predictions.\n",
    "      \n",
    "      The optimal model for prediction may not be suitable for inference\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple experiment for different bewteen inference and predictions\n",
    "\n",
    "x = np.random.rand(100)\n",
    "\n",
    "y = 2. * x.copy() + 1. # y is linearly related with x\n",
    "\n",
    "z = x ** 2 + 0.1 # z is linearly related with x^2\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(x,y,c='k')\n",
    "plt.scatter(x,z,c='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now consider in the real world, observations of x,y,z all come with errors\n",
    "# let's assume these errors are normal with zero means\n",
    "sigma_x = 0.1\n",
    "sigma_y = 0.1\n",
    "sigma_z = 0.1\n",
    "x_obs = x + np.random.randn(100) * sigma_x\n",
    "y_obs = y + np.random.randn(100) * sigma_y\n",
    "z_obs = z + np.random.randn(100) * sigma_z\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(x_obs,y_obs,c='k')\n",
    "plt.scatter(x_obs,z_obs,c='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try some regressions\n",
    "df_test = pd.DataFrame({'x' : x_obs,'y' : y_obs, 'z' : z_obs})\n",
    "\n",
    "# we use the first 50 data points to train the model\n",
    "# we then use the last 50 data to validate the model (out-of-sample test)\n",
    "\n",
    "df_train = df_test.iloc[0:50]\n",
    "df_valid = df_test.iloc[50::]\n",
    "\n",
    "res1 = smf.ols('y ~ x',data=df_train).fit()\n",
    "res2 = smf.ols('y ~ x + z',data=df_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res1.summary())\n",
    "print(res2.summary())\n",
    "print(anova_lm(res1,res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "Y1_predict = res1.predict(df_valid)\n",
    "Y2_predict = res2.predict(df_valid)\n",
    "\n",
    "# compare RMSE\n",
    "print('RMSE:')\n",
    "print('Model 1:',np.sqrt(np.mean(np.power(Y1_predict - df_valid['y'],2))))\n",
    "print('Model 2:',np.sqrt(np.mean(np.power(Y2_predict - df_valid['y'],2))))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-upper",
   "metadata": {},
   "source": [
    "#### 2. How to 'score' a model\n",
    "    \n",
    "  In order to rank/select models, we first need a method to score each model.\n",
    "    \n",
    "  Obviously, we have $R^2$, which denotes the fraction of variance explained by the independent variables. However, this metric has two potential problems: (1) R2 tends to always increase with new variables, adding to the risk of type I error; (2) R2 is ultimately a measure of 'explanatory' power of the model and has nothing to do with prediction, especially out-of-sample prediction.\n",
    "  \n",
    "  We briefly talked about $R^2_{adj}$ before, which includes some penalty of model complexity (number of independent variables) by considering the changes in degree of freedom when calculating $R^2$.\n",
    "  \n",
    "  Let's see a simple example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine how R2 and R2adj changes with increasing number of polynomial terms of dbh when explaining la\n",
    "\n",
    "# create the data columns\n",
    "df_ms['log_la'] = np.log(df_ms['la'])\n",
    "for i in range(1,10+1):\n",
    "    df_ms[f'log_dbh{i}'] = np.power(np.log(df_ms['dbh'].values),i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create regressions\n",
    "\n",
    "reg_ress = [] # store regression results\n",
    "reg_r2 = [] # store regression r2\n",
    "reg_r2adj = []# stored regression r2 adj\n",
    "\n",
    "for i in range(1,10+1):\n",
    "    x_strs = [f'log_dbh{j}' for j in range(1,i+1)]\n",
    "    \n",
    "    reg_str = 'log_la ~ ' + ' + '.join(x_strs)\n",
    "    \n",
    "    print(reg_str)\n",
    "    \n",
    "    res = smf.ols(reg_str,df_ms).fit()\n",
    "    \n",
    "    reg_ress.append(res)\n",
    "    reg_r2.append(res.rsquared)\n",
    "    reg_r2adj.append(res.rsquared_adj)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "x_val = range(1,10+1)\n",
    "plt.plot(x_val,reg_r2,'k-o')\n",
    "plt.plot(x_val,reg_r2adj,'r-s')\n",
    "\n",
    "# the model with maximum r2 and r2adj\n",
    "print(np.argmax(reg_r2))\n",
    "print(np.argmax(reg_r2adj))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the last model\n",
    "dbh_val = np.log(np.arange(0.001,1.,0.0001))\n",
    "data_predict = {}\n",
    "for i in range(1,10+1):\n",
    "    data_predict[f'log_dbh{i}'] = np.power(dbh_val,i)\n",
    "    \n",
    "Y_pred = reg_ress[3].predict(pd.DataFrame(data_predict))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(df_ms['log_dbh1'],df_ms['log_la'])\n",
    "plt.plot(dbh_val,Y_pred,'r-')\n",
    "plt.show()\n",
    "\n",
    "# r2_adj helps but is not perfect at excluding false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_ress[3].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-mustang",
   "metadata": {},
   "source": [
    "* Akaike Information Criterion (AIC)\n",
    "\n",
    "    Althought $R^2_{adj}$ does not work perfectly, the underlying motivation makes sense, which separates the model score into **goodness of fit** and **penalty of complexity**.\n",
    "    \n",
    "    AIC uses log-likelihood (recall what is likelihood - P(Y | model, X)) and number of model parameters to construct the score\n",
    "    \n",
    "    AIC = - ($log_e(L)$ - 2k), where k is the number of parameters. The negative sign upfront is made so that models with smaller AIC will be 'better'.\n",
    "    \n",
    "    AICc corrects for small sample size (e.g. < 20). AICc = AIC + $\\frac{2k^2+2k}{n-k-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_aic = [] # store regression AIC\n",
    "\n",
    "for i in range(1,10+1):\n",
    "    x_strs = [f'log_dbh{j}' for j in range(1,i+1)]\n",
    "    \n",
    "    reg_str = 'log_la ~ ' + ' + '.join(x_strs)\n",
    "    \n",
    "    print(reg_str)\n",
    "    \n",
    "    res = smf.ols(reg_str,df_ms).fit()\n",
    "    \n",
    "    reg_aic.append(res.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with another model\n",
    "res_h = smf.ols('log_la ~ log_dbh1 + np.log(h)',df_ms).fit()\n",
    "print(res_h.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "x_val = range(1,10+1)\n",
    "plt.plot(x_val,reg_aic,'b-d')\n",
    "plt.show()\n",
    "\n",
    "# the results look similar to R2_adj but they can differ in certain cases\n",
    "# AIC is more recommended\n",
    "# Check Burnham & Anderson 2004 for more details\n",
    "# 1. K. P. Burnham, D. R. Anderson, Multimodel inference - understanding AIC and BIC in model selection. Sociol. Methods Res. 33, 261–304 (2004)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-basket",
   "metadata": {},
   "source": [
    "* Cross-validation\n",
    "\n",
    "    If we are mostly interested in predictions, we can use **cross-validation** to estimate the predictive risk even without new data.\n",
    "    \n",
    "    Cross-validation separates the whole data set into *training* and *validation* data sets. A general method is called **K-fold cross validation**, we divide the data into k groups; often 5-10. We omit one group of data and fit the models to the remaining data. We use the fitted model to predict the data in the groups that was omitted (out-of-sample). Note that when k is equal to n, it becomes **leave-one-out cross validation**\n",
    "    \n",
    "    We can use RMSE to assess the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scikit learn package\n",
    "# a powerful package for machine learning and data analysis\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(5)\n",
    "kf_indexes = kf.split(df_ms)\n",
    "# create data\n",
    "kf_data = []\n",
    "for train_idx, test_idx in kf_indexes:\n",
    "    df_train = df_ms.iloc[train_idx]\n",
    "    df_test  = df_ms.iloc[test_idx]\n",
    "    \n",
    "    kf_data.append((df_train,df_test))\n",
    "\n",
    "# loop over models\n",
    "mod_cv_rmse = []\n",
    "for i in range(1,10+1):\n",
    "    x_strs = [f'log_dbh{j}' for j in range(1,i+1)]\n",
    "    \n",
    "    reg_str = 'log_la ~ ' + ' + '.join(x_strs)\n",
    "    \n",
    "    print(reg_str)\n",
    "    \n",
    "    y_pre = []\n",
    "    y_obs = []\n",
    "    \n",
    "    # loop over K-Fold groups\n",
    "    for df_train,df_test in kf_data:\n",
    "        \n",
    "        res = smf.ols(reg_str,df_train).fit()\n",
    "        y_obs.extend(df_test['log_la'].values)\n",
    "        y_pre.extend(res.predict(df_test))\n",
    "    \n",
    "    y_obs = np.array(y_obs)\n",
    "    y_pre = np.array(y_pre)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mod_cv_rmse.append(np.sqrt(np.mean(np.power(y_obs - y_pre,2))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "x_val = range(1,10+1)\n",
    "plt.plot(x_val,mod_cv_rmse,'b-d')\n",
    "plt.show()\n",
    "print(np.argmin(mod_cv_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-division",
   "metadata": {},
   "source": [
    "####  3. How to search for the best\n",
    "\n",
    "Now that we know a few metrics to score each statistical model, the next question is how to search through all the models to find the best one.\n",
    "\n",
    "The most naive option is to conduct an **exhaustive** search. This method can ensure we find the optimal model from all the candidate explanatory variables. However, If there are *k* covariates, there are $2^k$ possible models. Even if we only have 10 candidate variables (which can be easily exceeded for modeling ecological processes which integrate various physical, chemical, and biological processes), there will be 1024 models to evaluate. This approach becomes computationally consuming and even infeasible when k is very large.\n",
    "\n",
    "Naturally, we will consider trimming the unnecessary model evaluations. One common set of methods is **stepwise regression**. It can be both *forward* or *backward*. In forward stepwise regression, we start with no covariates and then add the one variable that leads to the best score (requires evaluation of *k* different models). We continue adding variables one at a time (always the one leading to the best score, which requires k-1, k-2, ..., evaluations) until the score does not improve. Backwards stepwise regression is the same except that we start with all possible covariates and drop one variable at a time. Both are 'greedy' searches (i.e. maximize the gain at the current time step) and thus neither is guaranteed to find the model with the best score because greedy searches ensure convergence to local optimal solutions but not necessarily global optimal solutions. Despite its limitation, these are probably the most common search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for model covariate/feature selection to predict la\n",
    "# use linear_regression from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# use Recursive Feature Elimination, largely equiavlent to backward stepwise selection\n",
    "# R has a similar step function as well\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Let's consider predict log_la\n",
    "# We will use 10 dbh variables (1-> 10th power), 10 h variables (1 -> 10th power),\n",
    "# 4 wd variables (wd, wd:dbh, wd:h, wd:dbh:h)\n",
    "# 2 climate variables, MAT and MAP\n",
    "\n",
    "df_rfe = df_ms[['log_la','mat','map','wd']]\n",
    "\n",
    "# create dbh and h vars\n",
    "for i in range(1,10+1):\n",
    "    df_rfe[f'log_dbh{i}'] = np.power(np.log(df_ms['dbh'].values),i)\n",
    "    df_rfe[f'log_h{i}'] = np.power(np.log(df_ms['h'].values),i)\n",
    "    \n",
    "# create wd interaction terms\n",
    "df_rfe['log_wd'] = np.log(df_rfe['wd'])\n",
    "df_rfe['log_wd_dbh'] = df_rfe['log_wd'] * df_rfe['log_dbh1']\n",
    "df_rfe['log_wd_h'] = df_rfe['log_wd'] * df_rfe['log_h1']\n",
    "df_rfe['log_wd_dbh_h'] = df_rfe['log_wd'] * df_rfe['log_dbh1'] *df_rfe['log_h1']\n",
    "\n",
    "print(df_rfe.shape)\n",
    "# have 27 covariates/features in total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use rfe to select 10 predictors\n",
    "rfe = RFECV(LinearRegression(),verbose=1)\n",
    "# get X\n",
    "x_str = df_rfe.columns.tolist()\n",
    "x_str.remove('log_la')\n",
    "\n",
    "rfe = rfe.fit(df_rfe[x_str].values,df_rfe['log_la'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "print(np.array(x_str)[rfe.support_])\n",
    "print(rfe.grid_scores_) # psuedo R2, 1- RSS/TSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-debut",
   "metadata": {},
   "source": [
    "* Regularization\n",
    "\n",
    "An alternative to stepwise model selection is to apply regression with **statistical regularization**. It includes *ridge regression, LASSO (least absolute shrinkage and selection operator), and the elastic net*.\n",
    "\n",
    "The idea behind regularization is similar to model scoring with consideration of both goodness of fit and model complexity. Instead of analyzing the score after fitting the model, statisticians developed method to directly include the model complexity penalty during model fitting so that we can the model fitting process can directly 'select features'.\n",
    "\n",
    "Some more details of regularization can be found in the supporting materials of Tredennick et al. (2021). In brief, for the case of OLS regression, instead of minimizing the squared residuals (good-ness of fit), we can also minimize the penalty associated with model complexity\n",
    "\n",
    "$loss function = \\sum_{i=1}{n} (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\gamma_1\\sum_{j=1}^{p} |\\beta_j|^{\\gamma_2}$.\n",
    "\n",
    "Here $\\gamma_1$ is referred to as the 'penalization' parameter determining the strenght of penalty towards more complex models.\n",
    "\n",
    "When $\\gamma_2$ is equal to 2, it becomes **Ridge regression**. This will penalize on models with too large coefficients (which is usually the case for model overfitting), which will lead to \"shrinkage\" of model parameters\n",
    "\n",
    "When $\\gamma_2$ is equal to 1, it becomes **LASSO**, which can shrink the coefficient estimates all the way to **zero**. This will lead to automatic feature selection during model fitting. The # of final selected covariates depend on the value of $\\gamma_1$\n",
    "\n",
    "**Elastic net** is a combination of Ridge regression and LASSO that includes both kinds of regulators with adjustable weights for each regulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge/LASSO regression for polynomial fit between la and dbh\n",
    "x_str = [f'log_dbh{i}' for i in range(1,10+1)]\n",
    "\n",
    "# first compare it with a linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# it is recommended to standarize all variables for Ridge/Lasso/ElasticNet\n",
    "# we do the same for linear regression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(df_ms[x_str + ['log_la']].values)\n",
    "data_scaled = scaler.transform(df_ms[x_str + ['log_la']].values)\n",
    "\n",
    "X_scaled = data_scaled[:,0:-1]\n",
    "y_scaled = data_scaled[:,-1]\n",
    "\n",
    "res_lin = LinearRegression().fit(X_scaled,y_scaled)\n",
    "print(res_lin.score(X_scaled,y_scaled)) # get R2\n",
    "print(res_lin.coef_) # get coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# test different alpha/gamma_1 values\n",
    "res_ridge = RidgeCV(alphas=np.logspace(-4,1,10)).fit(X_scaled, y_scaled)\n",
    "print(res_ridge.score(X_scaled, y_scaled)) # R2\n",
    "print(res_ridge.coef_)\n",
    "print(res_ridge.alpha_) # best alpha\n",
    "\n",
    "# test to use different alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.logspace(-4,1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "# test different alpha/gamma_1 values\n",
    "res_lasso = LassoCV(alphas=np.logspace(-4,1,10)).fit(X_scaled, y_scaled)\n",
    "print(res_lasso.score(X_scaled, y_scaled)) # R2\n",
    "print(res_lasso.coef_)\n",
    "print(res_lasso.alpha_) # best alpha\n",
    "\n",
    "# test using different alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results\n",
    "# visualize three different models\n",
    "dbh_val = np.log(np.arange(0.001,1.,0.0001))\n",
    "# the last one is dummy variable\n",
    "X = np.array([np.power(dbh_val,i) for i in range(1,11+1)]).T\n",
    "print(X.shape)\n",
    "\n",
    "X_scaled = scaler.transform(X)[:,0:10] # only the first 10\n",
    "    \n",
    "y_lin = res_lin.predict(X_scaled)\n",
    "y_ridge = res_ridge.predict(X_scaled)\n",
    "y_lasso = res_lasso.predict(X_scaled)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(scaler.transform(df_ms[x_str + ['log_la']].values)[:,0],y_scaled)\n",
    "plt.plot(X_scaled[:,0],y_lin,'k-',label='OLS')\n",
    "plt.plot(X_scaled[:,0],y_ridge,'r-',label='Ridge')\n",
    "plt.plot(X_scaled[:,0],y_lasso,'g-',label='LASSO')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in practice, try different values of alpha to get the best Cross Validation results\n",
    "# we will use elastic net as an example to select the best model to predict la\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# get X\n",
    "x_str = df_rfe.columns.tolist()\n",
    "x_str.remove('log_la')\n",
    "\n",
    "data_str = x_str + ['log_la'] # the last column is log_la\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(df_rfe[data_str].values)\n",
    "data_scaled = scaler.transform(df_rfe[data_str].values)\n",
    "\n",
    "X_scaled = data_scaled[:,0:-1]\n",
    "y_scaled = data_scaled[:,-1]\n",
    "\n",
    "res_ee = ElasticNetCV(l1_ratio=np.logspace(-4,0,20)).fit(X_scaled,y_scaled) # half way between ridge and lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.logspace(-4,0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_ee.alpha_) # the best alpha\n",
    "print(res_ee.l1_ratio_) # the best l1_ratio\n",
    "print(res_ee.score(X_scaled,y_scaled)) # best model score\n",
    "print(pd.DataFrame({'covariate': x_str,'coef': res_ee.coef_})) # best model coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-knitting",
   "metadata": {},
   "source": [
    "#### 4. Summary/Discussion:\n",
    "\n",
    "What would be a general procedure for model selection in practice?\n",
    "\n",
    "* exploration\n",
    "* inference\n",
    "* prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-logging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
